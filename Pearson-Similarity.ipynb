{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b84865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c6d8d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "362b7c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('pre_processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9dcbd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features (X)\n",
    "X_user = df[['researchExp', 'industryExp', 'internExp', 'journalPubs', 'confPubs', 'cgpa', 'gre_score', 'univName']]\n",
    "\n",
    "# Target (y)\n",
    "y_user = df['userName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b827897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       researchExp  industryExp  internExp  journalPubs  confPubs    cgpa  \\\n",
      "0                0           18        5.0            0         0  0.8500   \n",
      "1                0           66        0.0            0         0  0.7828   \n",
      "2                0            0        0.0            0         0  0.5700   \n",
      "3                0            0        0.0            0         0  0.6220   \n",
      "4                0            0        0.0            0         0  0.5200   \n",
      "...            ...          ...        ...          ...       ...     ...   \n",
      "25827            0            0        0.0            0         0  0.7400   \n",
      "25828            0            0        0.0            0         0  0.8200   \n",
      "25829            0            0        0.0            0         0  0.8400   \n",
      "25830            0            0        0.0            0         0  0.7200   \n",
      "25831            0            0        0.0            0         0  0.9160   \n",
      "\n",
      "       gre_score  univName  \n",
      "0            276        53  \n",
      "1            276        53  \n",
      "2            276        53  \n",
      "3            276        53  \n",
      "4            276        53  \n",
      "...          ...       ...  \n",
      "25827        300         0  \n",
      "25828        300         0  \n",
      "25829        300         0  \n",
      "25830        300         0  \n",
      "25831        300         0  \n",
      "\n",
      "[25832 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Assuming 'data' is your DataFrame containing the 'univName' column\n",
    "# Replace 'data' with your actual DataFrame\n",
    "\n",
    "# Initialize LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'univName' column\n",
    "X_user['univName'] = label_encoder.fit_transform(X_user['univName'])\n",
    "\n",
    "# Print the transformed DataFrame\n",
    "print(X_user)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb391c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Worcester Polytechnic Institute': 53, 'Wayne State University': 52, 'Virginia Polytechnic Institute and State University': 51, 'University of Wisconsin Madison': 50, 'University of Washington': 49, 'University of Utah': 48, 'University of Texas Dallas': 47, 'University of Texas Austin': 46, 'University of Texas Arlington': 45, 'University of Southern California': 44, 'University of Pennsylvania': 43, 'University of North Carolina Charlotte': 42, 'University of North Carolina Chapel Hill': 41, 'University of Minnesota Twin Cities': 40, 'University of Michigan Ann Arbor': 39, 'University of Massachusetts Amherst': 38, 'University of Maryland College Park': 37, 'University of Illinois Urbana-Champaign': 36, 'University of Illinois Chicago': 35, 'University of Florida': 34, 'University of Colorado Boulder': 33, 'University of Cincinnati': 32, 'University of California Santa Cruz': 31, 'University of California Santa Barbara': 30, 'University of California San Diego': 29, 'University of California Los Angeles': 28, 'University of California Irvine': 27, 'University of California Davis': 26, 'University of Arizona': 25, 'Texas A and M University College Station': 24, 'Syracuse University': 23, 'SUNY Stony Brook': 21, 'SUNY Buffalo': 20, 'Stanford University': 22, 'Rutgers University New Brunswick/Piscataway': 19, 'Purdue University': 18, 'Princeton University': 17, 'Ohio State University Columbus': 16, 'Northwestern University': 15, 'Northeastern University': 14, 'North Carolina State University': 13, 'New York University': 12, 'New Jersey Institute of Technology': 11, 'Massachusetts Institute of Technology': 10, 'Johns Hopkins University': 9, 'Harvard University': 8, 'Georgia Institute of Technology': 7, 'George Mason University': 6, 'Cornell University': 5, 'Columbia University': 4, 'Clemson University': 3, 'Carnegie Mellon University': 2, 'California Institute of Technology': 1, 'Arizona State University': 0}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Dictionary to store mappings\n",
    "encoded_values = {}\n",
    "\n",
    "# Open the CSV file\n",
    "with open('encoded_data.csv', 'r') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    \n",
    "    # Iterate over each row in the CSV file\n",
    "    for row in csv_reader:\n",
    "        # Get the unique entries for 'univName' and 'univName_encoded'\n",
    "        univ_name = row['univName']\n",
    "        encoded_value = row['univName_encoded']\n",
    "        \n",
    "        # Add the mapping to the dictionary if not already present\n",
    "        if univ_name not in encoded_values:\n",
    "            encoded_values[univ_name] = int(encoded_value)\n",
    "\n",
    "# Print the dictionary\n",
    "print(encoded_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3602cac",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m similarities \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_test)):\n\u001b[0;32m---> 13\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mappend([(idx, pearsonr(X_test\u001b[38;5;241m.\u001b[39miloc[i], X_train\u001b[38;5;241m.\u001b[39miloc[j])[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m idx, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train)))])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Sort similarities in descending order and get top 3 similar users for each test user\u001b[39;00m\n\u001b[1;32m     16\u001b[0m top_similar_users \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[9], line 13\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     11\u001b[0m similarities \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_test)):\n\u001b[0;32m---> 13\u001b[0m     similarities\u001b[38;5;241m.\u001b[39mappend([(idx, pearsonr(X_test\u001b[38;5;241m.\u001b[39miloc[i], X_train\u001b[38;5;241m.\u001b[39miloc[j])[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m idx, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_train)))])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Sort similarities in descending order and get top 3 similar users for each test user\u001b[39;00m\n\u001b[1;32m     16\u001b[0m top_similar_users \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/stats/_stats_py.py:4814\u001b[0m, in \u001b[0;36mpearsonr\u001b[0;34m(x, y, alternative, method)\u001b[0m\n\u001b[1;32m   4811\u001b[0m \u001b[38;5;66;03m# As explained in the docstring, the distribution of `r` under the null\u001b[39;00m\n\u001b[1;32m   4812\u001b[0m \u001b[38;5;66;03m# hypothesis is the beta distribution on (-1, 1) with a = b = n/2 - 1.\u001b[39;00m\n\u001b[1;32m   4813\u001b[0m ab \u001b[38;5;241m=\u001b[39m n\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 4814\u001b[0m dist \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mbeta(ab, ab, loc\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   4815\u001b[0m pvalue \u001b[38;5;241m=\u001b[39m _get_pvalue(r, dist, alternative)\n\u001b[1;32m   4817\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m PearsonRResult(statistic\u001b[38;5;241m=\u001b[39mr, pvalue\u001b[38;5;241m=\u001b[39mpvalue, n\u001b[38;5;241m=\u001b[39mn,\n\u001b[1;32m   4818\u001b[0m                       alternative\u001b[38;5;241m=\u001b[39malternative, x\u001b[38;5;241m=\u001b[39mx, y\u001b[38;5;241m=\u001b[39my)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/stats/_distn_infrastructure.py:851\u001b[0m, in \u001b[0;36mrv_generic.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m--> 851\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreeze(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/stats/_distn_infrastructure.py:846\u001b[0m, in \u001b[0;36mrv_generic.freeze\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Freeze the distribution for the given arguments.\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \n\u001b[1;32m    833\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    843\u001b[0m \n\u001b[1;32m    844\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, rv_continuous):\n\u001b[0;32m--> 846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rv_continuous_frozen(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rv_discrete_frozen(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/stats/_distn_infrastructure.py:465\u001b[0m, in \u001b[0;36mrv_frozen.__init__\u001b[0;34m(self, dist, *args, **kwds)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds \u001b[38;5;241m=\u001b[39m kwds\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# create a new instance\u001b[39;00m\n\u001b[0;32m--> 465\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdist\u001b[38;5;241m.\u001b[39m_updated_ctor_param())\n\u001b[1;32m    467\u001b[0m shapes, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist\u001b[38;5;241m.\u001b[39m_parse_args(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    468\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist\u001b[38;5;241m.\u001b[39m_get_support(\u001b[38;5;241m*\u001b[39mshapes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/stats/_distn_infrastructure.py:1833\u001b[0m, in \u001b[0;36mrv_continuous.__init__\u001b[0;34m(self, momtype, a, b, xtol, badvalue, name, longname, shapes, seed)\u001b[0m\n\u001b[1;32m   1828\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshapes \u001b[38;5;241m=\u001b[39m shapes\n\u001b[1;32m   1830\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_argparser(meths_to_inspect\u001b[38;5;241m=\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cdf],\n\u001b[1;32m   1831\u001b[0m                           locscale_in\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc=0, scale=1\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1832\u001b[0m                           locscale_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloc, scale\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1833\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_methods()\n\u001b[1;32m   1835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m longname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1836\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maeiouAEIOU\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/stats/_distn_infrastructure.py:1867\u001b[0m, in \u001b[0;36mrv_continuous._attach_methods\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;124;03mAttaches dynamically created methods to the rv_continuous instance.\u001b[39;00m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# _attach_methods is responsible for calling _attach_argparser_methods\u001b[39;00m\n\u001b[0;32m-> 1867\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attach_argparser_methods()\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# nin correction\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ppfvec \u001b[38;5;241m=\u001b[39m vectorize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ppf_single, otypes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/stats/_distn_infrastructure.py:696\u001b[0m, in \u001b[0;36mrv_generic._attach_argparser_methods\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03mGenerates the argument-parsing functions dynamically and attaches\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03mthem to the instance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;124;03mduring unpickling (__setstate__)\u001b[39;00m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    695\u001b[0m ns \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 696\u001b[0m exec(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_arg_template, ns)\n\u001b[1;32m    697\u001b[0m \u001b[38;5;66;03m# NB: attach to the instance, not class\u001b[39;00m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parse_args\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parse_args_stats\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parse_args_rvs\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "File \u001b[0;32m<string>:0\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fill NaN and infinity values with 0\n",
    "X_user.fillna(0, inplace=True)\n",
    "X_user.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "y_user.fillna(0, inplace=True)\n",
    "y_user.replace([np.inf, -np.inf], 0, inplace=True)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_user, y_user, test_size=0.2, random_state=42)\n",
    "\n",
    "# Compute Pearson correlation coefficient between test user and each user in the training data\n",
    "similarities = []\n",
    "for i in range(len(X_test)):\n",
    "    similarities.append([(idx, pearsonr(X_test.iloc[i], X_train.iloc[j])[0]) for idx, j in enumerate(range(len(X_train)))])\n",
    "\n",
    "# Sort similarities in descending order and get top 3 similar users for each test user\n",
    "top_similar_users = []\n",
    "for sims in similarities:\n",
    "    sims.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_similar_users.append(sims[:3])\n",
    "\n",
    "# Print the top 3 similar users for each test user\n",
    "for test_user, similar_users in zip(y_test, top_similar_users):\n",
    "    print(f\"Test User: {test_user}, Top 3 Similar Users:\")\n",
    "    for idx, (user_idx, similarity) in enumerate(similar_users):\n",
    "        print(f\"   {idx+1}. User Index: {user_idx}, Similarity: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b952734b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
